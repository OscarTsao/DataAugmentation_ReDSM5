pretrained_model_name: FacebookAI/roberta-base
classifier_hidden_sizes: []
classifier_dropout: 0.1
max_seq_length: 256
warmup_ratio: 0.1
learning_rate: 2e-5
weight_decay: 0.01
optimizer: adamw_torch
scheduler: linear
batch_size: 32
eval_batch_size: null  # If null, uses batch_size for eval. Can be larger since no gradients
gradient_accumulation_steps: 1
num_epochs: 5
adam_eps: 1e-8
max_grad_norm: 1.0
compile_model: false
